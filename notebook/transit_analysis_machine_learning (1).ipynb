{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TESS Transit Classification — Random Forest (scikit-learn)\n",
        "\n",
        "This notebook adapts the original deep-learning pipeline to a **Random Forest** classifier using **scikit-learn**.\n",
        "It keeps the same end-to-end structure: data loading, **balanced augmentation**, **threshold optimization** (don’t default to 0.5), and interpretable **AUC-first** evaluation.\n",
        "\n",
        "**Why try a Random Forest?**\n",
        "- Strong **nonlinear baseline** for high-dimensional tabular data (flux bins as features).\n",
        "- **Fast to train**, robust to feature scaling, and relatively easy to interpret (feature importances).\n",
        "- Useful as a **sanity-check** or production-friendly alternative to heavier neural networks.\n",
        "\n",
        "**What stays the same**\n",
        "- The dataset schema and split strategy.\n",
        "- Balanced augmentation to battle class imbalance.\n",
        "- AUC-focused evaluation and **Youden’s J** threshold selection from the ROC curve.\n",
        "- Visual diagnostics: confusion matrix, ROC/PR curves, and optional light-curve plots with predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Requirements & Data Schema\n",
        "\n",
        "Install if needed:\n",
        "```bash\n",
        "pip install numpy pandas scikit-learn matplotlib joblib\n",
        "```\n",
        "\n",
        "**Expected CSV (`tess_data.csv` by default):**\n",
        "- Flux bins: `flux_0000, flux_0001, ..., flux_0999` (or up to `n_bins - 1`)\n",
        "- Flux errors: `flux_err_0000, ..., flux_err_0999`\n",
        "- Label: `label` (0 = Non-Planet, 1 = Planet)\n",
        "- Metadata (for nicer plot titles): `toi_name, tic, disp, period_d, t0_bjd, dur_hr, sector`\n",
        "\n",
        "> You can change the file path and number of bins in the config cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Imports & Reproducibility\n",
        "\n",
        "We keep plotting **Matplotlib-only**. Random Forests don’t require feature scaling, but we’ll include it as an optional toggle for comparability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting working directory to:  /Users/tiziano/Dropbox/Computational Astrophysics/25_26/Kepler and TESS Classification/final\n",
            "None\n",
            "Environment ready.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import IPython\n",
        "    working_directory = \"/\".join(\n",
        "            IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"].split(\"/\")[:-1]\n",
        "        )\n",
        "    print(\"Setting working directory to: \", working_directory)\n",
        "    print(os.chdir(working_directory))\n",
        "except Exception as e:\n",
        "    print(\"It was impossible to set your directory as the current one because of the following message\")\n",
        "    print(e)\n",
        "    print(\"The working directory is: \", os.getcwd())\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Environment ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Configuration\n",
        "\n",
        "- `CSV_PATH`: dataset location  \n",
        "- `N_BINS`: number of flux bins per sample  \n",
        "- `USE_SCALER`: whether to standardize features (RF does not need it, but it’s harmless and keeps plotting consistent)  \n",
        "- `SAMPLES_PER_CLASS`: target count per class for **balanced augmentation** (only for the training split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_PATH = 'tess_data.csv'    # change if needed\n",
        "N_BINS = 1000\n",
        "USE_SCALER = False                  # Random Forest doesn't need scaling; set True if you want comparability\n",
        "SAMPLES_PER_CLASS = 350             # per-class size after augmentation (train split only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Balanced Augmentation\n",
        "\n",
        "We create a **perfectly balanced** training set via light augmentations (noise, scale, shift, combo).  \n",
        "This prevents the model from being overwhelmed by the majority class during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_balanced_dataset(X, y, samples_per_class=400):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CREATING BALANCED DATASET\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    X0 = X[y == 0]\n",
        "    X1 = X[y == 1]\n",
        "    print(f\"Original - Class 0: {len(X0)}, Class 1: {len(X1)}\")\n",
        "    \n",
        "    def augment_to_target(X_orig, n_target):\n",
        "        if len(X_orig) >= n_target:\n",
        "            idx = np.random.choice(len(X_orig), n_target, replace=False)\n",
        "            return X_orig[idx]\n",
        "        \n",
        "        X_result = [X_orig]\n",
        "        while len(np.vstack(X_result)) < n_target:\n",
        "            n_needed = n_target - len(np.vstack(X_result))\n",
        "            idx = np.random.choice(len(X_orig), min(len(X_orig), n_needed))\n",
        "            aug_type = np.random.rand()\n",
        "            if aug_type < 0.25:\n",
        "                X_aug = X_orig[idx] + np.random.normal(0, 0.01, (len(idx), X_orig.shape[1]))\n",
        "            elif aug_type < 0.5:\n",
        "                scale = 1.0 + np.random.uniform(-0.03, 0.03, (len(idx), 1))\n",
        "                X_aug = X_orig[idx] * scale\n",
        "            elif aug_type < 0.75:\n",
        "                shifts = np.random.randint(-20, 20, len(idx))\n",
        "                X_aug = np.array([np.roll(X_orig[i], s) for i, s in zip(idx, shifts)])\n",
        "            else:\n",
        "                X_aug = X_orig[idx] * (1.0 + np.random.uniform(-0.02, 0.02, (len(idx), 1)))\n",
        "                X_aug += np.random.normal(0, 0.008, X_aug.shape)\n",
        "            X_result.append(X_aug)\n",
        "        X_final = np.vstack(X_result)\n",
        "        return X_final[:n_target]\n",
        "    \n",
        "    X0_bal = augment_to_target(X0, samples_per_class)\n",
        "    X1_bal = augment_to_target(X1, samples_per_class)\n",
        "    print(f\"Balanced - Class 0: {len(X0_bal)}, Class 1: {len(X1_bal)}\")\n",
        "    \n",
        "    X_bal = np.vstack([X0_bal, X1_bal])\n",
        "    y_bal = np.concatenate([np.zeros(samples_per_class), np.ones(samples_per_class)])\n",
        "    \n",
        "    idx = np.arange(len(X_bal))\n",
        "    np.random.shuffle(idx)\n",
        "    return X_bal[idx], y_bal[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Data Loading & Split\n",
        "\n",
        "We **split first** (to avoid leakage), then apply **balanced augmentation only to the training set**.  \n",
        "Scaling is optional for Random Forests; set `USE_SCALER=True` to standardize using the training fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(csv_path='tess_data.csv', n_bins=1000, use_scaler=False, samples_per_class=350):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LOADING DATA\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Dataset: {df.shape[0]} samples\")\n",
        "    \n",
        "    flux_cols = [f'flux_{i:04d}' for i in range(n_bins)]\n",
        "    flux_err_cols = [f'flux_err_{i:04d}' for i in range(n_bins)]\n",
        "    X = df[flux_cols].values\n",
        "    X_err = df[flux_err_cols].values\n",
        "    y = df['label'].values\n",
        "    \n",
        "    metadata_cols = ['toi_name', 'tic', 'label', 'disp', 'period_d', 't0_bjd', 'dur_hr', 'sector']\n",
        "    metadata = df[metadata_cols]\n",
        "    \n",
        "    print(\"Original distribution:\")\n",
        "    print(f\"  Class 0: {(y==0).sum()}, Class 1: {(y==1).sum()}\")\n",
        "    if (y==0).sum() > 0:\n",
        "        print(f\"  Ratio: {(y==1).sum() / (y==0).sum():.2f}:1\")\n",
        "    \n",
        "    X_train, X_test, y_train, y_test, X_err_train, X_err_test, idx_train, idx_test = train_test_split(\n",
        "        X, y, X_err, np.arange(len(y)),\n",
        "        test_size=0.2,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=y\n",
        "    )\n",
        "    print(f\"Initial split - Train: {len(X_train)}, Test: {len(X_test)}\")\n",
        "    \n",
        "    # Balance training set\n",
        "    X_train, y_train = create_balanced_dataset(X_train, y_train, samples_per_class=samples_per_class)\n",
        "    \n",
        "    scaler = None\n",
        "    if use_scaler:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STANDARDIZATION\")\n",
        "        print(\"=\"*70)\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        print(f\"Train: mean={X_train.mean():.6f}, std={X_train.std():.6f}\")\n",
        "        print(f\"Test:  mean={X_test.mean():.6f}, std={X_test.std():.6f}\")\n",
        "    \n",
        "    metadata_test = metadata.iloc[idx_test].reset_index(drop=True)\n",
        "    print(f\"Final - X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "    print(f\"Train dist: 0={(y_train==0).sum()}, 1={(y_train==1).sum()}\")\n",
        "    \n",
        "    # Return X_test copy (for optional inverse transform plotting if scaler is used)\n",
        "    return X_train, X_test, y_train, y_test, metadata_test, X_test.copy(), X_err_test, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Random Forest Classifier\n",
        "\n",
        "We use a moderately large forest and enable `oob_score=True` for an out‑of‑bag estimate (quick internal validation).  \n",
        "Because we already balance the training set, we keep `class_weight=None` by default.  \n",
        "If you want to rely on the raw, imbalanced data instead, you can set `class_weight='balanced'`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_random_forest(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    class_weight=None,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    oob_score=True\n",
        "):\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        bootstrap=bootstrap,\n",
        "        class_weight=class_weight,\n",
        "        random_state=random_state,\n",
        "        n_jobs=n_jobs,\n",
        "        oob_score=oob_score\n",
        "    )\n",
        "    return rf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Train\n",
        "\n",
        "Fit the Random Forest on the **balanced** training set. We’ll print the OOB score if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train):\n",
        "    model.fit(X_train, y_train)\n",
        "    if hasattr(model, 'oob_score_') and model.oob_score_ is not None:\n",
        "        print(f\"OOB Score: {model.oob_score_:.4f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Threshold Optimization & Evaluation (AUC-first)\n",
        "\n",
        "- Compute probabilities with `predict_proba`  \n",
        "- Build the ROC curve and choose **Youden’s J** maximizing threshold  \n",
        "- Report AUC, default‑threshold accuracy (0.5), and optimal‑threshold accuracy  \n",
        "- Show a classification report at the optimal threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_optimal_threshold(model, X_test, y_test):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"THRESHOLD OPTIMIZATION & EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    proba = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, proba)\n",
        "    j_scores = tpr - fpr\n",
        "    best_idx = np.argmax(j_scores)\n",
        "    best_thresh = thresholds[best_idx]\n",
        "    \n",
        "    y_pred_default = (proba >= 0.5).astype(int)\n",
        "    y_pred_best = (proba >= best_thresh).astype(int)\n",
        "    \n",
        "    auc = roc_auc_score(y_test, proba)\n",
        "    acc_default = accuracy_score(y_test, y_pred_default)\n",
        "    acc_best = accuracy_score(y_test, y_pred_best)\n",
        "    \n",
        "    print(f\"Optimal threshold: {best_thresh:.4f} (default=0.5)\")\n",
        "    print(f\"  At this threshold: TPR={tpr[best_idx]:.4f}, FPR={fpr[best_idx]:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "    print(f\"Accuracy @0.5: {acc_default:.4f} ({acc_default*100:.2f}%)\")\n",
        "    print(f\"Accuracy @{best_thresh:.4f}: {acc_best:.4f} ({acc_best*100:.2f}%)\")\n",
        "    \n",
        "    print(\"\\nClassification report (optimal threshold):\")\n",
        "    print(classification_report(y_test, y_pred_best, target_names=['Non-Planet','Planet'], digits=4, zero_division=0))\n",
        "    \n",
        "    return y_pred_best, proba, best_thresh, (fpr, tpr, thresholds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Visualization (Matplotlib-only)\n",
        "\n",
        "We generate **one figure per chart** (no subplots) and avoid specifying colors.\n",
        "\n",
        "- Confusion matrix image with counts and percentages  \n",
        "- ROC curve  \n",
        "- Precision‑Recall curve  \n",
        "- Probability histograms per class  \n",
        "- Optional: sample light‑curve images (one figure per sample) with predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_image(y_true, y_pred, threshold, save_path='confusion_matrix_rf.png'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig = plt.figure(figsize=(6, 5))\n",
        "    ax = plt.gca()\n",
        "    im = ax.imshow(cm, interpolation='nearest')\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(2),\n",
        "           yticks=np.arange(2),\n",
        "           xticklabels=['Non-Planet', 'Planet'],\n",
        "           yticklabels=['Non-Planet', 'Planet'],\n",
        "           xlabel='Predicted', ylabel='True',\n",
        "           title=f'Confusion Matrix (threshold={threshold:.3f})')\n",
        "    total = cm.sum()\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            count = cm[i, j]\n",
        "            pct = (count / total * 100) if total > 0 else 0.0\n",
        "            ax.text(j, i, f\"{count}\\n({pct:.1f}%)\", ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_roc_curve(y_true, proba, save_path='roc_curve_rf.png'):\n",
        "    fpr, tpr, _ = roc_curve(y_true, proba)\n",
        "    auc = roc_auc_score(y_true, proba)\n",
        "    fig = plt.figure(figsize=(6, 5))\n",
        "    ax = plt.gca()\n",
        "    ax.plot(fpr, tpr, linewidth=2)\n",
        "    ax.plot([0,1], [0,1], linestyle='--')\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title(f'ROC Curve (AUC={auc:.4f})')\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_pr_curve(y_true, proba, save_path='pr_curve_rf.png'):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, proba)\n",
        "    ap = average_precision_score(y_true, proba)\n",
        "    fig = plt.figure(figsize=(6, 5))\n",
        "    ax = plt.gca()\n",
        "    ax.plot(recall, precision, linewidth=2)\n",
        "    ax.set_xlabel('Recall')\n",
        "    ax.set_ylabel('Precision')\n",
        "    ax.set_title(f'Precision-Recall Curve (AP={ap:.4f})')\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_probability_histograms(y_true, proba, save_path='probability_hist_RF.png'):\n",
        "    fig = plt.figure(figsize=(6, 5))\n",
        "    ax = plt.gca()\n",
        "    ax.hist(proba[y_true==0], bins=30, alpha=0.6, label='Non-Planet', density=True)\n",
        "    ax.hist(proba[y_true==1], bins=30, alpha=0.6, label='Planet', density=True)\n",
        "    ax.set_xlabel('Predicted Probability (class=1)')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title('Predicted Probability Distributions by Class')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_lightcurve_sample(idx, X_test_standardized, X_err_test, metadata_test, scaler=None, proba=None, y_true=None, y_pred=None, save_prefix='sample_lightcurve_RF'):\n",
        "    # Make a single-figure plot for one sample (no subplots)\n",
        "    x_std = X_test_standardized[idx].reshape(1, -1)\n",
        "    if scaler is not None:\n",
        "        x_orig = scaler.inverse_transform(x_std).flatten()\n",
        "        yerr = X_err_test[idx]\n",
        "    else:\n",
        "        x_orig = x_std.flatten()\n",
        "        yerr = X_err_test[idx]\n",
        "    \n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "    ax = plt.gca()\n",
        "    ax.errorbar(np.arange(len(x_orig)), x_orig, yerr=yerr, fmt='o', markersize=2, alpha=0.6)\n",
        "    ax.axhline(np.median(x_orig), linestyle='--', linewidth=1)\n",
        "    ax.set_xlabel('Time Bin')\n",
        "    ax.set_ylabel('Flux')\n",
        "    \n",
        "    toi = metadata_test.loc[idx, 'toi_name']\n",
        "    tic = metadata_test.loc[idx, 'tic']\n",
        "    disp = metadata_test.loc[idx, 'disp']\n",
        "    sector = metadata_test.loc[idx, 'sector']\n",
        "    \n",
        "    tstr = f'TOI {toi} (TIC {tic}, {disp}) - Sector {sector}'\n",
        "    if proba is not None and y_true is not None and y_pred is not None:\n",
        "        pred_str = 'Transit' if y_pred[idx]==1 else 'Non-Transit'\n",
        "        true_str = 'Transit' if y_true[idx]==1 else 'Non-Transit'\n",
        "        tstr += f'\\nTrue: {true_str} | Pred: {pred_str} (p={proba[idx]:.3f})'\n",
        "    \n",
        "    ax.set_title(tstr)\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    path = f\"{save_prefix}_{idx}.png\"\n",
        "    plt.savefig(path, dpi=300)\n",
        "    print(f\"Saved: {path}\")\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Run the Pipeline\n",
        "\n",
        "Run the following cells step-by-step, or use the **one‑click** `main()` at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING DATA\n",
            "======================================================================\n",
            "Dataset: 944 samples\n",
            "Original distribution:\n",
            "  Class 0: 472, Class 1: 472\n",
            "  Ratio: 1.00:1\n",
            "Initial split - Train: 755, Test: 189\n",
            "\n",
            "======================================================================\n",
            "CREATING BALANCED DATASET\n",
            "======================================================================\n",
            "Original - Class 0: 377, Class 1: 378\n",
            "Balanced - Class 0: 350, Class 1: 350\n",
            "Final - X_train: (700, 1000), X_test: (189, 1000)\n",
            "Train dist: 0=350, 1=350\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X_train, X_test, y_train, y_test, metadata_test, X_test_std_copy, X_err_test, scaler = load_data(\n",
        "    csv_path=CSV_PATH, n_bins=N_BINS, use_scaler=USE_SCALER, samples_per_class=SAMPLES_PER_CLASS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build RF model\n",
        "rf = build_random_forest(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    class_weight=None,     # already balanced via augmentation\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    oob_score=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOB Score: 0.8086\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "rf = train_model(rf, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "THRESHOLD OPTIMIZATION & EVALUATION\n",
            "======================================================================\n",
            "Optimal threshold: 0.3960 (default=0.5)\n",
            "  At this threshold: TPR=0.9787, FPR=0.2526\n",
            "AUC-ROC: 0.9046\n",
            "Accuracy @0.5: 0.8466 (84.66%)\n",
            "Accuracy @0.3960: 0.8624 (86.24%)\n",
            "\n",
            "Classification report (optimal threshold):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Non-Planet     0.9726    0.7474    0.8452        95\n",
            "      Planet     0.7931    0.9787    0.8762        94\n",
            "\n",
            "    accuracy                         0.8624       189\n",
            "   macro avg     0.8829    0.8630    0.8607       189\n",
            "weighted avg     0.8833    0.8624    0.8606       189\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate & choose best threshold\n",
        "y_pred_opt, proba_test, best_thresh, roc_tuple = evaluate_with_optimal_threshold(rf, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: confusion_matrix_rf.png\n",
            "Saved: roc_curve_rf.png\n",
            "Saved: pr_curve_rf.png\n",
            "Saved: probability_hist_RF.png\n"
          ]
        }
      ],
      "source": [
        "# Visualizations\n",
        "plot_confusion_matrix_image(y_test, y_pred_opt, best_thresh, save_path='confusion_matrix_rf.png')\n",
        "plot_roc_curve(y_test, proba_test, save_path='roc_curve_rf.png')\n",
        "plot_pr_curve(y_test, proba_test, save_path='pr_curve_rf.png')\n",
        "plot_probability_histograms(y_test, proba_test, save_path='probability_hist_RF.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: sample_lightcurve_RF_114.png\n",
            "Saved: sample_lightcurve_RF_174.png\n",
            "Saved: sample_lightcurve_RF_54.png\n",
            "Saved: sample_lightcurve_RF_49.png\n"
          ]
        }
      ],
      "source": [
        "# Optional: save a handful of single-sample light curves with predictions\n",
        "# (No subplots; one figure per sample.)\n",
        "num_samples = min(4, len(X_test_std_copy))\n",
        "idxs = np.random.choice(len(X_test_std_copy), num_samples, replace=False)\n",
        "for i in idxs:\n",
        "    plot_lightcurve_sample(\n",
        "        i, X_test_std_copy, X_err_test, metadata_test,\n",
        "        scaler=scaler, proba=proba_test, y_true=y_test, y_pred=y_pred_opt,\n",
        "        save_prefix='sample_lightcurve_RF'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: tess_rf_model.joblib, rf_optimal_threshold.npy\n"
          ]
        }
      ],
      "source": [
        "# Persist the model and the optimal threshold\n",
        "joblib.dump(rf, 'tess_rf_model.joblib')\n",
        "np.save('rf_optimal_threshold.npy', best_thresh)\n",
        "print(\"Saved: tess_rf_model.joblib, rf_optimal_threshold.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) One‑Click: `main()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    X_train, X_test, y_train, y_test, metadata_test, X_test_std_copy, X_err_test, scaler = load_data(\n",
        "        csv_path=CSV_PATH, n_bins=N_BINS, use_scaler=USE_SCALER, samples_per_class=SAMPLES_PER_CLASS\n",
        "    )\n",
        "    rf = build_random_forest(\n",
        "        n_estimators=500,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='sqrt',\n",
        "        bootstrap=True,\n",
        "        class_weight=None,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        oob_score=True\n",
        "    )\n",
        "    rf = train_model(rf, X_train, y_train)\n",
        "    y_pred_opt, proba_test, best_thresh, roc_tuple = evaluate_with_optimal_threshold(rf, X_test, y_test)\n",
        "    plot_confusion_matrix_image(y_test, y_pred_opt, best_thresh, save_path='confusion_matrix_rf.png')\n",
        "    plot_roc_curve(y_test, proba_test, save_path='roc_curve_rf.png')\n",
        "    plot_pr_curve(y_test, proba_test, save_path='pr_curve_rf.png')\n",
        "    plot_probability_histograms(y_test, proba_test, save_path='probability_hist_RF.png')\n",
        "    \n",
        "    # Optional: preview a few light curves\n",
        "    num_samples = min(4, len(X_test_std_copy))\n",
        "    idxs = np.random.choice(len(X_test_std_copy), num_samples, replace=False)\n",
        "    for i in idxs:\n",
        "        plot_lightcurve_sample(\n",
        "            i, X_test_std_copy, X_err_test, metadata_test,\n",
        "            scaler=scaler, proba=proba_test, y_true=y_test, y_pred=y_pred_opt,\n",
        "            save_prefix='sample_lightcurve_RF'\n",
        "        )\n",
        "    \n",
        "    joblib.dump(rf, 'tess_rf_model.joblib')\n",
        "    np.save('rf_optimal_threshold.npy', best_thresh)\n",
        "    print(\"Artifacts saved: tess_rf_model.joblib, rf_optimal_threshold.npy, and PNG figures.\")\n",
        "\n",
        "# Uncomment to run end-to-end\n",
        "# main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Next Steps\n",
        "\n",
        "- **Hyperparameter tuning**: Try `RandomizedSearchCV` or `GridSearchCV` on `max_depth`, `min_samples_leaf`, `max_features`, `n_estimators`.\n",
        "- **Class weight instead of augmentation**: Set `class_weight='balanced'` and disable augmentation to compare strategies.\n",
        "- **Calibration**: Use `CalibratedClassifierCV` to improve probability calibration (often improves thresholding).\n",
        "- **Feature engineering**: Derive summary stats (depth, duration proxies, local trends) to reduce dimensionality and boost RF performance.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
